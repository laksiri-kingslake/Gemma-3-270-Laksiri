{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20a4ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./venv/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in ./venv/lib/python3.10/site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./venv/lib/python3.10/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c36ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./venv/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./venv/lib/python3.10/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in ./venv/lib/python3.10/site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283ccbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/my/llm/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887b1b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7dda49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.10/site-packages (from tiktoken) (2025.7.34)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.10/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd19537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=8,\n",
    "        )\n",
    "    # concatenate all the ids in each dataset into one large file we can use for training\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            # Batch together samples for faster write\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            # Write into mmap\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd434f0",
   "metadata": {},
   "source": [
    "### Step 3: Create Input-Output batches for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c22dbdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl (888.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.0/888.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 KB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.8.90\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.8.93\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.8.93\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.4.0\n",
      "  Downloading triton-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.3.83\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.9.90\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.8.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.27.3\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.8.90\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.13.1.3\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.8.93\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in ./venv/lib/python3.10/site-packages (from triton==3.4.0->torch) (59.6.0)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.8.0 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "699fc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
    "#block size = context window\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6b411",
   "metadata": {},
   "source": [
    "### Step 4: Define the SLM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f4389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5bbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587b6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # Gemma3 stores zero-centered weights and uses (1 + weight) during forward\n",
    "        self.scale = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Match HF Gemma3: compute norm in float32, then scale by (1 + w)\n",
    "        input_dtype = x.dtype\n",
    "        x_f = x.float()\n",
    "        var = x_f.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = x_f * torch.rsqrt(var + self.eps)\n",
    "        out = x_norm * (1.0 + self.scale.float())\n",
    "\n",
    "        if self.shift is not None:\n",
    "            out = out + self.shift.float()\n",
    "\n",
    "        return out.to(input_dtype)\n",
    "\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False,\n",
    "        query_pre_attn_scalar=None, dtype=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "        if query_pre_attn_scalar is not None:\n",
    "            self.scaling = (query_pre_attn_scalar) ** -0.5\n",
    "        else:\n",
    "            self.scaling = (head_dim) ** -0.5\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Apply projections\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Optional normalization\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys = self.k_norm(keys)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin)\n",
    "        keys = apply_rope(keys, cos, sin)\n",
    "\n",
    "        # Expand K and V to match number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Scale queries\n",
    "        queries = queries * self.scaling\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.gelu(x_fc1, approximate=\"tanh\") * x_fc2\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: dict, attn_type: str):\n",
    "        super().__init__()\n",
    "        self.attn_type = attn_type\n",
    "\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            query_pre_attn_scalar=cfg[\"query_pre_attn_scalar\"],\n",
    "            dtype=cfg[\"dtype\"],\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.input_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.post_attention_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.pre_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.post_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        mask_global,\n",
    "        mask_local,\n",
    "        cos_global,\n",
    "        sin_global,\n",
    "        cos_local,\n",
    "        sin_local,\n",
    "    ):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.input_layernorm(x)\n",
    "\n",
    "        if self.attn_type == \"sliding_attention\":\n",
    "            attn_mask = mask_local\n",
    "            cos = cos_local\n",
    "            sin = sin_local\n",
    "        else:\n",
    "            attn_mask = mask_global\n",
    "            cos = cos_global\n",
    "            sin = sin_global\n",
    "\n",
    "        x_attn = self.att(x, attn_mask, cos, sin)\n",
    "        x_attn = self.post_attention_layernorm(x_attn)\n",
    "        x = shortcut + x_attn\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x_ffn = self.pre_feedforward_layernorm(x)\n",
    "        x_ffn = self.ff(x_ffn)\n",
    "        x_ffn = self.post_feedforward_layernorm(x_ffn)\n",
    "        x = shortcut + x_ffn\n",
    "        return x\n",
    "\n",
    "class Gemma3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        assert cfg[\"layer_types\"] is not None and len(cfg[\"layer_types\"]) == cfg[\"n_layers\"]\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg, attn_type)for attn_type in cfg[\"layer_types\"]\n",
    "        ])\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Reusuable utilities\n",
    "        cos_local, sin_local = compute_rope_params(\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            theta_base=cfg[\"rope_local_base\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        cos_global, sin_global = compute_rope_params(\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.register_buffer(\"cos_local\", cos_local, persistent=False)\n",
    "        self.register_buffer(\"sin_local\", sin_local, persistent=False)\n",
    "        self.register_buffer(\"cos_global\", cos_global, persistent=False)\n",
    "        self.register_buffer(\"sin_global\", sin_global, persistent=False)\n",
    "\n",
    "    def _create_masks(self, seq_len, device):\n",
    "        ones = torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)\n",
    "\n",
    "        # mask_global (future is masked: j > i)\n",
    "        #     j:  0 1 2 3 4 5 6 7\n",
    "        #  i\n",
    "        #     0:  0 1 1 1 1 1 1 1\n",
    "        #     1:  0 0 1 1 1 1 1 1\n",
    "        #     2:  0 0 0 1 1 1 1 1\n",
    "        #     3:  0 0 0 0 1 1 1 1\n",
    "        #     4:  0 0 0 0 0 1 1 1\n",
    "        #     5:  0 0 0 0 0 0 1 1\n",
    "        #     6:  0 0 0 0 0 0 0 1\n",
    "        #     7:  0 0 0 0 0 0 0 0\n",
    "        mask_global = torch.triu(ones, diagonal=1)\n",
    "\n",
    "        # far_past (too far back is masked: i - j >= sliding_window)\n",
    "        # where sliding_window = 4\n",
    "        #     j:  0 1 2 3 4 5 6 7\n",
    "        #  i\n",
    "        #     0:  0 0 0 0 0 0 0 0\n",
    "        #     1:  0 0 0 0 0 0 0 0\n",
    "        #     2:  0 0 0 0 0 0 0 0\n",
    "        #     3:  0 0 0 0 0 0 0 0\n",
    "        #     4:  1 0 0 0 0 0 0 0\n",
    "        #     5:  1 1 0 0 0 0 0 0\n",
    "        #     6:  1 1 1 0 0 0 0 0\n",
    "        #     7:  1 1 1 1 0 0 0 0\n",
    "        far_past = torch.triu(ones, diagonal=self.cfg[\"sliding_window\"]).T\n",
    "\n",
    "        # Local (sliding_window) = future OR far-past\n",
    "        # mask_local\n",
    "        #     j:  0 1 2 3 4 5 6 7\n",
    "        # i\n",
    "        # 0:      0 1 1 1 1 1 1 1\n",
    "        # 1:      0 0 1 1 1 1 1 1\n",
    "        # 2:      0 0 0 1 1 1 1 1\n",
    "        # 3:      0 0 0 0 1 1 1 1\n",
    "        # 4:      1 0 0 0 0 1 1 1\n",
    "        # 5:      1 1 0 0 0 0 1 1\n",
    "        # 6:      1 1 1 0 0 0 0 1\n",
    "        # 7:      1 1 1 1 0 0 0 0\n",
    "        mask_local = mask_global | far_past\n",
    "        return mask_global, mask_local\n",
    "\n",
    "    def forward(self, input_ids, targets=None):\n",
    "        b, seq_len = input_ids.shape\n",
    "        x = self.tok_emb(input_ids) * (self.cfg[\"emb_dim\"] ** 0.5)\n",
    "        mask_global, mask_local = self._create_masks(seq_len, x.device)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(\n",
    "                x,\n",
    "                mask_global=mask_global,\n",
    "                mask_local=mask_local,\n",
    "                cos_global=self.cos_global,\n",
    "                sin_global=self.sin_global,\n",
    "                cos_local=self.cos_local,\n",
    "                sin_local=self.sin_local,\n",
    "            )\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "      for _ in range(max_new_tokens):\n",
    "        ctx_len = self.cfg[\"context_length\"]\n",
    "        idx_cond = idx if idx.size(1) <= ctx_len else idx[:, -ctx_len:]\n",
    "        logits, _ = self(idx_cond)  # targets=None by default\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = float(\"-inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "      return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74280eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMMA3_CONFIG_270M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 32_768,\n",
    "    \"emb_dim\": 640,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 18,\n",
    "    \"hidden_dim\": 2048,\n",
    "    \"head_dim\": 256,\n",
    "    \"qk_norm\": True,\n",
    "    \"n_kv_groups\": 1,\n",
    "    \"rope_local_base\": 10_000.0,\n",
    "    \"rope_base\": 1_000_000.0,\n",
    "    \"sliding_window\": 512,\n",
    "      \"layer_types\": [\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\"\n",
    "    ],\n",
    "    \"dtype\": torch.bfloat16,\n",
    "    \"query_pre_attn_scalar\": 256,\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = Gemma3Model(GEMMA3_CONFIG_270M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e999191",
   "metadata": {},
   "source": [
    "### Step 5: Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d511555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8866c7b2",
   "metadata": {},
   "source": [
    "### Step 6: Define SLM Training Configuration Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfbc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7188c3b4ea10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Config\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
    "max_iters = 150000 #increase from 25000\n",
    "warmup_steps = 1000 #smoother initial train, earlier 100\n",
    "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
    "eval_iters = 500 # increased from 100\n",
    "batch_size = 32 # changed from 16, better gradient estimate\n",
    "block_size = 128 #changed from 64, capture longer range dependencies\n",
    "\n",
    "gradient_accumulation_steps = 32 # reduced from 50\n",
    "\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "\n",
    "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
    "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049794f",
   "metadata": {},
   "source": [
    "### Step 7: Define SLM Training Configuration Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "991f2ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2197/2132813893.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "\n",
    "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
    "\n",
    "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b9615",
   "metadata": {},
   "source": [
    "### Step 8: Pre-train the SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 500/1500 [11:49<23:39,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: train loss 8.3707, val loss 8.3782\n",
      "The current learning rate: 0.00007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 999/1500 [31:34<11:51,  1.42s/it]    /home/ubuntu/my/llm/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      " 67%|██████▋   | 1000/1500 [31:35<11:50,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: train loss 7.0041, val loss 7.0026\n",
      "The current learning rate: 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [51:21<00:00,  2.05s/it]    \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# In your training loop\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        # Ensure estimate_loss uses the correct device\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    # Ensure X and y are on the correct device\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291602ca",
   "metadata": {},
   "source": [
    "### Step 9: Plot the SLM Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bccada78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m131.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 KB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Downloading pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m141.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23 in ./venv/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.59.1 kiwisolver-1.4.9 matplotlib-3.10.5 pillow-11.3.0 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a106f77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZiNJREFUeJzt3XmcTfXjx/HXvbMziyFmTIaxZN/3JTshyVIRwtjXRKnITtmSForIVrbwRULWKFuIGRTGvpQZyjbWGTP38/tDza/JOpqZM8v7+Xicx6N7zuee874nue/Oco/NGGMQERERSSPsVgcQERERSUwqNyIiIpKmqNyIiIhImqJyIyIiImmKyo2IiIikKSo3IiIikqao3IiIiEia4mx1gOTmcDg4e/YsXl5e2Gw2q+OIiIjIIzDGcPXqVQICArDbH3xsJt2Vm7NnzxIYGGh1DBEREXkMZ86cIUeOHA8ck+7KjZeXF3Bn53h7e1ucRkRERB5FZGQkgYGBcd/jD5Luys3fp6K8vb1VbkRERFKZR7mkRBcUi4iISJqiciMiIiJpisqNiIiIpCnp7pobERFJHLGxsdy+fdvqGJKGuLq6PvQ270ehciMiIglijCEiIoLLly9bHUXSGLvdTu7cuXF1df1P67G03MTGxjJs2DDmzJlDREQEAQEBBAcHM2jQoEe6Gnrr1q1Ur16dokWLEhoamvSBRUQkrthky5aNDBky6AdRJVH8/SO74eHh5MyZ8z/9ubK03IwdO5bJkycze/ZsihQpws8//0z79u3x8fGhd+/eD3zv5cuXadu2LbVr1+bcuXPJlFhEJH2LjY2NKzZZsmSxOo6kMVmzZuXs2bPExMTg4uLy2OuxtNxs27aNxo0b07BhQwCCgoKYP38+O3fufOh7u3XrRqtWrXBycmLZsmVJnFRERIC4a2wyZMhgcRJJi/4+HRUbG/ufyo2ld0tVrlyZDRs2cPjwYQD27t3Lli1baNCgwQPfN3PmTI4fP87QoUMfuo2oqCgiIyPjTSIi8t/oVJQkhcT6c2XpkZv+/fsTGRlJwYIFcXJyIjY2lvfee4/WrVvf9z1Hjhyhf//+bN68GWfnh8cfPXo0w4cPT8zYIiIikoJZeuRm4cKFzJ07l3nz5rFnzx5mz57N+PHjmT179j3Hx8bG0qpVK4YPH07+/PkfaRsDBgzgypUrcdOZM2cS8yOIiIhICmMzxhirNh4YGEj//v3p2bNn3Lx3332XOXPmcOjQobvGX758GV9fX5ycnOLmORwOjDE4OTmxdu1aatWq9cBtRkZG4uPjw5UrV/RsKRGRBLp16xYnTpwgd+7cuLu7Wx3HMkFBQfTp04c+ffr853Vt2rSJmjVrcunSJTJlyvSf15eaPejPV0K+vy09LXXjxo27fqzHyckJh8Nxz/He3t7s378/3rzPPvuM77//nsWLF5M7d+4ky/pINmyAihUhY0Zrc4iIyF1q1KhByZIl+eijj/7zunbt2kVG/V2fYllabho1asR7771Hzpw5KVKkCCEhIUyYMIEOHTrEjRkwYAC///47X375JXa7naJFi8ZbR7Zs2XB3d79rfrILCYFnn4V8+WDhQihSxNo8IiKSIMYYYmNjH+l6zqxZsyZDInlcll5zM3HiRF588UV69OhBoUKF6NevH127dmXkyJFxY8LDwzl9+rSFKR/RjRtc9/aAAwegXDmYMQOsO+MnIpJsjDFcj76e7FNCrqoIDg7mhx9+4OOPP8Zms2Gz2Zg1axY2m43vvvuOMmXK4ObmxpYtWzh27BiNGzfGz88PT09PypUrx/r16+OtLygoKN4RIJvNxhdffEHTpk3JkCEDTz31FMuXL3/sffq///2PIkWK4ObmRlBQEB988EG85Z999hlPPfUU7u7u+Pn58eKLL8YtW7x4McWKFcPDw4MsWbJQp04drl+//thZUiNLj9x4eXnx0UcfPfAQ4axZsx64jmHDhjFs2LBEzfU4VmS9RIfgK8z/xpnaR25Cx46wcSNMngyenlbHExFJMjdu38BzdPL/PXdtwDUyuj7aqaGPP/6Yw4cPU7RoUUaMGAHAr7/+Cty5c3f8+PHkyZMHX19fzpw5w7PPPst7772Hm5sbX375JY0aNSIsLIycOXPedxvDhw9n3LhxvP/++0ycOJHWrVtz6tQpMmfOnKDPtXv3bpo3b86wYcNo0aIF27Zto0ePHmTJkoXg4GB+/vlnevfuzVdffUXlypW5ePEimzdvBu4cEGjZsiXjxo2jadOmXL16lc2bNyeoCKYFerZUIimStQh5ClSgbssdvL0V3t1ow2nOHNi5885pqhIlrI4oIpJu+fj44OrqSoYMGfD39weIu3FlxIgR1K1bN25s5syZKfGPv7NHjhzJ0qVLWb58Ob169brvNoKDg2nZsiUAo0aN4pNPPmHnzp3Ur18/QVknTJhA7dq1GTx4MAD58+fnwIEDvP/++wQHB3P69GkyZszIc889h5eXF7ly5aJUqVLAnXITExNDs2bNyJUrFwDFihVL0PbTApWbRJLbNzeb229m4PcDGWN/n805Df9b6oLf4cNQoQJ8/DF06QL64SsRSWMyuGTg2oBrlmw3MZQtWzbe62vXrjFs2DBWrlwZVxZu3rz50EskihcvHvfPGTNmxNvbm/Pnzyc4z8GDB2ncuHG8eVWqVOGjjz4iNjaWunXrkitXLvLkyUP9+vWpX79+3OmwEiVKULt2bYoVK0a9evV45plnePHFF/H19U1wjtTM0mtu0hoXJxfG1R3HqlarCCv0BEU632Z1ASeIioJu3aBlS9AvJItIGmOz2cjomjHZp8T6Ndt/3/XUr18/li5dyqhRo9i8eTOhoaEUK1aM6OjoB67n348LsNls973797/w8vJiz549zJ8/n+zZszNkyBBKlCjB5cuXcXJyYt26dXz33XcULlyYiRMnUqBAAU6cOJHoOVIylZsk0OCpBoR2DaVI4Wo82yKWfnUhxskGX38NpUvDnj1WRxQRSXdcXV2JjY196LitW7cSHBxM06ZNKVasGP7+/pw8eTLpA/6lUKFCbN269a5M+fPnj/udN2dnZ+rUqcO4cePYt28fJ0+e5PvvvwfulKoqVaowfPhwQkJCcHV1ZenSpcmWPyXQaakk8qT3k2xou4GRP4xkpH0kW3IalixxIeDYMahUCT74AHr21GkqEZFkEhQUxI4dOzh58iSenp73Pary1FNPsWTJEho1aoTNZmPw4MFJcgTmft544w3KlSvHyJEjadGiBdu3b2fSpEl89tlnAKxYsYLjx49TrVo1fH19WbVqFQ6HgwIFCrBjxw42bNjAM888Q7Zs2dixYwd//PEHhQoVSrb8KYGO3CQhZ7szw2sOZ0PbDZwulJ2inW/zbSE7REfDq6/Ciy/C5ctWxxQRSRf69euHk5MThQsXJmvWrPe9hmbChAn4+vpSuXJlGjVqRL169ShdunSy5SxdujQLFy5kwYIFFC1alCFDhjBixAiCg4MByJQpE0uWLKFWrVoUKlSIKVOmMH/+fIoUKYK3tzc//vgjzz77LPnz52fQoEF88MEHD30gdVpj6eMXrGDV4xfOXz9P26VtWXN0Db13wAfr7TjHOCAo6M7pqvLlky2LiMjj0uMXJCkl1uMXdOQmmWTLmI1VrVcxtu5YPq3kRKX2Ds5kcYaTJ+Hpp+HDD/WjfyIiIolA5SYZ2W123qryFj+2/5HzhXNStHMM/ytig9u34fXXoUkTuHjR6pgiIpKIunXrhqen5z2nbt26WR0vTdJpKYtcvHmRjss7suzgMrrvgo/X2nGJcUBgICxYAJUrW5ZNROR+dFoq4c6fP0/kfX4GxNvbm2zZsiVzopQrTTwVPD3L7JGZJc2XMGnnJPo592N7YDRL/udM7jNnoFo1GDUK+vUDuw6uiYikZtmyZVOBSWb65rSQzWbj1Qqvsr3jdq4VyUfxzjHML2aD2Fh4+2147jn44w+rY4qIiKQqKjcpQOnspdnTZQ+NyrSkVTND50YQ5WKH776DkiXhrweiiYiIyMOp3KQQXm5ezG02ly+e/4K5FTwo19HB0axOcPYs1KgB770HyfgjUiIiIqmVyk0KYrPZ6Fi6I7s67yK2WGFKdorly+LcKTWDBkH9+nDunNUxRUREUjSVmxSoSLYi7Oq8i5YVO9GuGQQ3hluudli37s5pqr+eHyIiIiJ3U7lJoTK4ZGDa89OY12we/6vgSelODg76OUFEBNSpA8OG3bnwWEREkkVQUBAfffRR3GubzcayZcvuO/7kyZPYbDZCQ0P/03YTaz0J8bDPltKp3KRwLYu1ZE+XPbgXL0WZjrF8UYo7v2Q8fDjUrQvh4VZHFBFJl8LDwxP9mU3BwcE0adIk3rzAwEDCw8MpWrRoom4rLVO5SQWeyvIU2ztup9PTr9K5MbRuBjfc7LBx453TVOvWWR1RRCTd8ff3x83NLcm34+TkhL+/P87O+mm6R6Vyk0q4ObvxSYNPWNpiKavKZ6JUZwe/+Nvh/HmoV+/OBccxMVbHFJH0yBi4fj35pwT8wP7UqVMJCAjA8a+7Ths3bkyHDh04duwYjRs3xs/PD09PT8qVK8f69esfuM5/n7rZuXMnpUqVwt3dnbJlyxISEhJvfGxsLB07diR37tx4eHhQoEABPv7447jlw4YNY/bs2XzzzTfYbDZsNhubNm2652mpH374gfLly+Pm5kb27Nnp378/Mf/4DqhRowa9e/fmrbfeInPmzPj7+zNs2LBH3l//tn//fmrVqoWHhwdZsmShS5cuXLt2LW75pk2bKF++PBkzZiRTpkxUqVKFU6dOAbB3715q1qyJl5cX3t7elClThp9//vmxszwKlZtUpknBJoR2DSVLyUqU6+hgclnu/Af+3ntQqxb89pvVEUUkvblxAzw9k3+6ceORI7700ktcuHCBjRs3xs27ePEiq1evpnXr1ly7do1nn32WDRs2EBISQv369WnUqBGnT59+pPVfu3aN5557jsKFC7N7926GDRtGv3794o1xOBzkyJGDRYsWceDAAYYMGcI777zDwoULAejXrx/Nmzenfv36hIeHEx4eTuV7PIrn999/59lnn6VcuXLs3buXyZMnM336dN59991442bPnk3GjBnZsWMH48aNY8SIEax7jCP9169fp169evj6+rJr1y4WLVrE+vXr6dWrFwAxMTE0adKE6tWrs2/fPrZv306XLl2w2WwAtG7dmhw5crBr1y52795N//79cXFxSXCOBDHpzJUrVwxgrly5YnWU/yQ6Jtr0X9ffMAzT/EXMVXe7MWBMlizGrFxpdTwRSaNu3rxpDhw4YG7evPn/M69du/P3T3JP164lKHvjxo1Nhw4d4l5//vnnJiAgwMTGxt5zfJEiRczEiRPjXufKlct8+OGHca8Bs3Tp0rh1ZcmSJd5+mTx5sgFMSEjIfTP17NnTvPDCC3Gv27VrZxo3bhxvzIkTJ+Kt55133jEFChQwDocjbsynn35qPD094z5L9erVzdNPPx1vPeXKlTNvv/32fbP80z8/29SpU42vr6+59o/9vXLlSmO3201ERIS5cOGCAcymTZvuuS4vLy8za9asR9ruPf98/SUh3986cpNKuTi5MLrOaFa3Xs3G8lkp2dlBSIAdLlyAhg3hrbfuPG1cRCSpZcgA164l/5QhQ4Jitm7dmv/9739ERUUBMHfuXF5++WXsdjvXrl2jX79+FCpUiEyZMuHp6cnBgwcf+cjNwYMHKV68eLyHPVaqVOmucZ9++illypQha9aseHp6MnXq1Efexj+3ValSpbgjIwBVqlTh2rVr/PaPo/fFixeP977s2bNz/vz5BG3r7+2VKFGCjBkzxtuew+EgLCyMzJkzExwcTL169WjUqBEff/wx4f+42eX111+nU6dO1KlThzFjxnDs2LEEZ0golZtUrl6+eoR2CyVnmZpU7ODgk/J/LXj/faheHRL4H42ISILZbJAxY/JP//hyfxSNGjXCGMPKlSs5c+YMmzdvpnXr1sCdU0JLly5l1KhRbN68mdDQUIoVK0Z0dHSi7aYFCxbQr18/OnbsyNq1awkNDaV9+/aJuo1/+vepH5vNdtc1R4ll5syZbN++ncqVK/P111+TP39+fvrpJ+DOtUS//vorDRs25Pvvv6dw4cIsXbo0SXL8TeUmDQjwCmBdm3UMrDOcvg3tNGsOkR522L79zt1Uy5dbHVFExHLu7u40a9aMuXPnMn/+fAoUKEDp0qUB2Lp1K8HBwTRt2pRixYrh7+/PyZMnH3ndhQoVYt++fdy6dStu3t9f7n/bunUrlStXpkePHpQqVYp8+fLddRTD1dWV2If8hlmhQoXYvn075h8XVG/duhUvLy9y5MjxyJkfVaFChdi7dy/Xr1+Ptz273U6BAgXi5pUqVYoBAwawbds2ihYtyrx58+KW5c+fn759+7J27VqaNWvGzJkzEz3nP6ncpBFOdieGVB/C922/Z0eFAEp0cfDzkza4dAkaN4a+fSGJ/u9ARCS1aN26NStXrmTGjBlxR20AnnrqKZYsWUJoaCh79+6lVatWCTrK0apVK2w2G507d+bAgQOsWrWK8ePHxxvz1FNP8fPPP7NmzRoOHz7M4MGD2bVrV7wxQUFB7Nu3j7CwMP78809u3+Pygh49enDmzBleffVVDh06xDfffMPQoUN5/fXXsdsT/2u9devWuLu7065dO3755Rc2btzIq6++Sps2bfDz8+PEiRMMGDCA7du3c+rUKdauXcuRI0coVKgQN2/epFevXmzatIlTp06xdetWdu3aRaFChRI95z+p3KQx1YOqE9o1lMLln6Vye8MHf5/y/egjePppOHHCyngiIpaqVasWmTNnJiwsjFatWsXNnzBhAr6+vlSuXJlGjRpRr169uKM6j8LT05Nvv/2W/fv3U6pUKQYOHMjYsWPjjenatSvNmjWjRYsWVKhQgQsXLtCjR494Yzp37kyBAgUoW7YsWbNmZevWrXdt68knn2TVqlXs3LmTEiVK0K1bNzp27MigQYMSuDceTYYMGVizZg0XL16kXLlyvPjii9SuXZtJkybFLT906BAvvPAC+fPnp0uXLvTs2ZOuXbvi5OTEhQsXaNu2Lfnz56d58+Y0aNCA4cOHJ0nWv9nMP49rpQORkZH4+Phw5coVvL29rY6TZBzGwYfbP6T/hv7UPxjDV9/YyXTDAT4+MGMGNGtmdUQRSYVu3brFiRMnyJ07d7yLZ0USw4P+fCXk+1tHbtIou83OG5XfYEv7LfxSIYjiXRxsD7TBlSvwwgvw6qvwj3PDIiIiaYXKTRpXIUcFQrqGUL7SC1QLNoyt8teCSZOgcmU4etTSfCIikrzmzp2Lp6fnPaciRYpYHS9R6EEV6UAm90wsemkRk3NP5nXX19kUFMWcZXayhIRA6dIwdSq8/LLVMUVEJBk8//zzVKhQ4Z7LkvyXg5OJyk06YbPZ6FGuB5UDK9NicQuK+x1mwf+g6qmr0LLlnYdwfvQReHhYHVVERJKQl5cXXl5eVsdIUjotlc6U9C/J7i67qfX0K9RsCyOrgcPGnaM3FStCWJjVEUUkFUiqH4OT9C2x7nHSkZt0yNPVky+bfEnt3LXp6d6TH3PdYN5SO1n37YMyZWDKFHjlFatjikgK5Orqit1u5+zZs2TNmhVXV9d4jwEQeVzGGP744w9sNtt/Pj2mW8HTuYN/HKT54ub8eewX5v4Pap38a0H79ncuOk7gs1tEJO2Ljo4mPDycGwl4KrfIo7DZbOTIkQNPT8+7liXk+1vlRrh5+yZ91/Rl2q7PGfQjDP0B7AYoXBgWLoQ0cvW8iCQeYwwxMTEPfVSASEK4uLjg5OR0z2UqNw+gcnN/C39dSOdvO1P6UCTzl9jwv2ruXGD86acQHJzgh9SJiIgkFv2InzyW5kWas6fLHq5WLkPxroY1eYGbN6FDB2jXDq5dszqiiIjIQ1labmJjYxk8eDC5c+fGw8ODvHnzMnLkyAdeLb1kyRLq1q1L1qxZ8fb2plKlSqxZsyYZU6dteTPnZWuHrbSu3YcGreGdWhBrA776CsqWhX37rI4oIiLyQJaWm7FjxzJ58mQmTZrEwYMHGTt2LOPGjWPixIn3fc+PP/5I3bp1WbVqFbt376ZmzZo0atSIkJCQZEyetrk5u/Fh/Q/5ptVyPq+XmRrB8Lu37c5t4hUq3LltPH2dzRQRkVTE0mtunnvuOfz8/Jg+fXrcvBdeeAEPDw/mzJnzyOspUqQILVq0YMiQIXcti4qKIioqKu51ZGQkgYGBuubmEZ25coaW/2vJoUNbmb0MGh75a8HLL8Pnn4P2oYiIJINUc81N5cqV2bBhA4cPHwZg7969bNmyhQYNGjzyOhwOB1evXiVz5sz3XD569Gh8fHzipsDAwETJnl4E+gSyKXgT3eoP5PmW8GZduG0HFiy485s4OmImIiIpjKVHbhwOB++88w7jxo3DycmJ2NhY3nvvPQYMGPDI6xg3bhxjxozh0KFDZMuW7a7lOnKTeNYdW8crS18hz6HzfP0/GzkvG3B1hQkToEcP3U0lIiJJJtUcuVm4cCFz585l3rx57Nmzh9mzZzN+/Hhmz579SO+fN28ew4cPZ+HChfcsNgBubm54e3vHm+Tx1M1bl73d9pKxWm1KdjF8UwCIjoZeveCll+DyZasjioiIWHvkJjAwkP79+9OzZ8+4ee+++y5z5szh0KFDD3zvggUL6NChA4sWLaJhw4aPvE39zs1/F+uIZcyWMQzZOJhXtxvGrQfXWCB3bvj6ayhXzuqIIiKSxqSaIzc3btzAbo8fwcnJ6aEPZJs/fz7t27dn/vz5CSo2kjic7E4MrDaQH9r/yP/q5aBKBzjhC5w4galS5c7TxXU3lYiIWMTSctOoUSPee+89Vq5cycmTJ1m6dCkTJkygadOmcWMGDBhA27Zt417PmzePtm3b8sEHH1ChQgUiIiKIiIjgypUrVnyEdO3pnE8T2jUU/5rPUaoLLC4Ettu3oW9faNIELl60OqKIiKRDlp6Wunr1KoMHD2bp0qWcP3+egIAAWrZsyZAhQ3B1dQUgODiYkydPsmnTJgBq1KjBDz/8cNe62rVrx6xZsx66TZ2WSnzGGD766SPeXvcWnXfEMGENuMUCOXPeuauqUiWrI4qISCqnZ0s9gMpN0tn1+y5aLG6Bz8ETLFwET10E4+yMbdQoeOMNsOtpHyIi8nhSzTU3kraUe7IcIV1DyF+nOWW6wPyiYIuJgbfegkaN4M8/rY4oIiLpgMqNJCofdx8WvLCA91+YQocWbnR5Dm45A6tWQcmSsHmz1RFFRCSNU7mRRGez2ehatis7Ou9kc/2ClO8Eh54Afv8dU7MmjBoFD7kjTkRE5HGp3EiSKe5XnF2dd1G6XjvKdoYvi4MtNhYGDoT69eH8easjiohIGqRyI0nK09WTWU1mMbnFl/RonoH2jeGGC7BuHZQoARs3Wh1RRETSGJUbSRZtSrRhd9c97KlfnHKd4desQEQEpk4dGD4cYmOtjigiImmEyo0kmwJPFGBHpx3UeLYH5TrD9FJgczhg2DB45hkID7c6ooiIpAEqN5Ks3J3d+bThp3zZehGvN/fmlaZwzRX4/vs7d1OtW2d1RBERSeVUbsQSLxZ+kdCuoRx5tjxlO8NeP+D8eUy9ejBoEMTEWB1RRERSKZUbsUxu39xsbr+Z555/g4qdYEoZsBkD770HtWrB779bHVFERFIhlRuxlKuTK+OfGc/itisY1DwLL78AkW7c+bG/kiXhu++sjigiIqmMyo2kCA3zNyS0Wyjhz1WjdBfY48+dxzU8+yy8/Tbcvm11RBERSSVUbiTFyOGdgw1tN9C66RCqdISJ5f9aMG4cVK8Op09bmk9ERFIHlRtJUZztzgyvOZwVHdYzqrk/LzSHy+7A9u2YkiVh+XKrI4qISAqnciMpUu08tQntGsq1Rs9QugvsDADbpUvQuDG8/jpER1sdUUREUiiVG0mx/Dz9+K71d3R5aTTVO9r5sOJfCz78EKpWhRMnLM0nIiIpk8qNpGh2m53+T/dnQ+fNfNQiJ8+/DBc9gJ07MaVKwZIlVkcUEZEURuVGUoXKgZUJ6RqCvXFjSnWFbTnAduUKvPACvPoqREVZHVFERFIIlRtJNTJ7ZGZpi6W88fLH1O3kwtgqfy2YNAkqV4ajRy3NJyIiKYPKjaQqNpuN3hV682Pn7UxrnpdnW8GfGYA9ezClS8PChVZHFBERi6ncSKpUJqAMe7ruwafZy5TsCptzgu3qVWjRArp1g5s3rY4oIiIWUbmRVMvbzZt5zeYx7JVpPNvRnXergsMGfP45VKwIYWFWRxQREQuo3EiqZrPZ6FS6E9u77WL+y4Wp9wqczwjs24cpUwbmzLE6ooiIJDOVG0kTimYrys5OO8n5QgdKdIPvg8B2/Tq0aQMdO8KNG1ZHFBGRZKJyI2lGRteMTG88nQ/azaVpp4wMq/7XaaoZM6B8eThwwOqIIiKSDFRuJM1pVawVP3cLYXnLUtRpA+GewK+/YsqWhVmzrI4nIiJJTOVG0qSnsjzF9o7bKfryq5TsBmvzgO3mTWjfHtq1g2vXrI4oIiJJROVG0iw3Zzc+afAJUzos4eVOPrxTC2JtwJdfQrlysH+/1RFFRCQJqNxImte0UFP2dA9lY6uK1AiG37yAQ4cw5cvDtGlgjNURRUQkEancSLoQlCmIH4N/pHLLtyjZDVblA9utW9ClC7RqBZGRVkcUEZFEonIj6YaLkwtj645lTpfvaN/pCd6sCzF2YMECKFMGQkKsjigiIolA5UbSnfr56hPSYy8/t65B1fZw2hs4ehRTsSJ89plOU4mIpHIqN5IuBXgFsL7Nehq0GU7p7ja+KQC26Gjo2ROaN4crV6yOKCIij0nlRtItJ7sTQ6oPYXG37+neyZ++9SDaCVi8GFOqFOzaZXVEERF5DCo3ku7VCKrB3u77CGvTgKfbw4lMYDtxAlOlCnz8sU5TiYikMio3IkDWjFlZ0WoFLwWPo1x3J/5XCGy3b0OfPtC0KVy8aHVEERF5RCo3In+x2+y8WeVNVvTYwhudc9KrAUQ5Ad98c+c01U8/WR1RREQegcqNyL9UzFGRkG6hhAc3o1JHOOoLttOnMVWrwvjx4HBYHVFERB7A0nITGxvL4MGDyZ07Nx4eHuTNm5eRI0diHnKNw6ZNmyhdujRubm7ky5ePWXoYoiQyXw9fFr+0mI4dJ1GxhwsLioAtJgbefBOefx7+/NPqiCIich+WlpuxY8cyefJkJk2axMGDBxk7dizjxo1j4sSJ933PiRMnaNiwITVr1iQ0NJQ+ffrQqVMn1qxZk4zJJT2w2Wz0LN+TdT13MKRLPro8B7ecgZUr75ym2rLF6ogiInIPNvOwwyRJ6LnnnsPPz4/p06fHzXvhhRfw8PBgzpw593zP22+/zcqVK/nll1/i5r388stcvnyZ1atX3zU+KiqKqKiouNeRkZEEBgZy5coVvL29E/HTSFp2Neoq3Vd2Z9+6uSxaBAUugHFywjZyJLz9Nth1hldEJClFRkbi4+PzSN/flv6NXLlyZTZs2MDhw4cB2Lt3L1u2bKFBgwb3fc/27dupU6dOvHn16tVj+/bt9xw/evRofHx84qbAwMDE+wCSbni5efFV06/o22UGVXt68FVxsMXGwjvvQIMGcP681RFFROQvlpab/v378/LLL1OwYEFcXFwoVaoUffr0oXXr1vd9T0REBH5+fvHm+fn5ERkZyc2bN+8aP2DAAK5cuRI3nTlzJtE/h6QPNpuN9qXas6nXz4zrWoQOz8MNZ2DtWkzJkrBpk8UJRUQELC43CxcuZO7cucybN489e/Ywe/Zsxo8fz+zZsxNtG25ubnh7e8ebRP6LwlkLs6PzTpw7daZcF/g1K9jCwzG1a8OIERAba3VEEZF0zdJy8+abb8YdvSlWrBht2rShb9++jB49+r7v8ff359y5c/HmnTt3Dm9vbzw8PJI6sggAGVwyMLXRVAZ3n0/tnp7MKAk2hwOGDoVnnoGICKsjioikW5aWmxs3bmD/14WYTk5OOB7wOyKVKlViw4YN8eatW7eOSpUqJUlGkQd5uejLbHk1hM+6l6FNU7jmAnz/PaZECVi/3up4IiLpkqXlplGjRrz33nusXLmSkydPsnTpUiZMmEDTpk3jxgwYMIC2bdvGve7WrRvHjx/nrbfe4tChQ3z22WcsXLiQvn37WvERRMiXOR9bO2zliS59KNsF9mUD2/nzmGeegcGDISbG6ogiIumKpbeCX716lcGDB7N06VLOnz9PQEAALVu2ZMiQIbi6ugIQHBzMyZMn2fSPizU3bdpE3759OXDgADly5GDw4MEEBwc/0jYTciuZSEItD1tOt0XtGLrsMl13/zWzWjWYNw+efNLSbCIiqVlCvr8tLTdWULmRpHb6ymla/q8lgau2Me1b8IoG88QT2L76CurXtzqeiEiqlGp+50YkLcrpk5NN7TaRp/sAynSFPf5g+/PPO7+H078/3L5tdUQRkTRN5UYkCbg4uTCq9igmvbaGJr2zMqncXwvGjoUaNUC/tyQikmRUbkSS0DN5n2Hnq/tY1qs2L74EV9yAbdswJUvAt99aHU9EJE1SuRFJYv6e/qx5ZQ2ler1LmW42dgWA7eKlO08Xf+MNiI62OqKISJqiciOSDJzsTgysNpAZfTfR/LUAPqz414IJEzBVq8KJE5bmExFJS1RuRJJRtVzV2NVrLxtebUjjl+GSO9h27sSUKglLl1odT0QkTVC5EUlmT2R4gm9bfkv13h9Qrocz23OA7UokNGsGvXtDVJTVEUVEUjWVGxEL2Gw2Xq/0OvNe30q7vkGMq/zXgokTMVWqwLFjluYTEUnNVG5ELFT+yfLs6hHKrj4v0rAV/OkBtt27cZQqCQsXWh1PRCRVUrkRsZiPuw8LX1xIo76TqdDTlc05wX71GrRoAd27w61bVkcUEUlVVG5EUgCbzUa3st1Y8sZOur2en/eqgsMGTJmCqVgRDh+2OqKISKqhciOSgpTwL8GO7rs58no76reG8xnAtncvjtKlYO5cq+OJiKQKKjciKYynqyezmszilX6zqfyqBxuDwH79BrzyCnTqBDduWB1RRCRFU7kRSaHalmjLin57eP3NYgyrDg6A6dMx5cvDgQNWxxMRSbFUbkRSsIJPFGRb5x2c69eNOm0h3BNsv/6Ko1xZmDXL6ngiIimSyo1ICufh4sHk5ybT/a2FVO3tybo8YL9xE9q3h3bt4No1qyOKiKQoKjciqcRLRV5ibb+9DH6rLANrQawN+PLLO0dx9u+3Op6ISIqhciOSiuTxzcOPnbZy663XqdkOfvcC+6EwHOXLwRdfgDFWRxQRsZzKjUgq4+rkygf1PuCtAd9Ss08mvssH9ltR0LkztG4NV69aHVFExFIqNyKp1HP5n+P7fvsZ0/9p3qoDMTZg/nwcpUtDaKjV8URELKNyI5KK5fDOwYb2G3EbMIjq7eG0N9iPHsVRsQJMnqzTVCKSLqnciKRyznZnRtYayYiB66n3elaW5wd7VDT06IFp0QKuXLE6oohIslK5EUkjauepzaY39jNpYB361oPbdrAtWnTnCeM//2x1PBGRZKNyI5KG+Hn6sbrNGrINHEW1jnZOZAL7iZM4KleCjz/WaSoRSRdUbkTSGLvNzoCqAxg/6EcavfkkSwqC/XYM9OmDadYMLl2yOqKISJJSuRFJo6rkrMKPffcxe0gjejWAKCewLVtGbMkSsGOH1fFERJKMyo1IGpbZIzPLXv6Gp4Z8RLXOzhz1BafTZ3A8XQU++AAcDqsjiogkOpUbkTTOZrPxWsXX+HTwdl7oH8TXRcAeEwv9+mGebwQXLlgdUUQkUanciKQTZQPK8mPvUJYObU7X5+CWE9hWriK2RHHYutXqeCIiiUblRiQd8XH3Yf6LCyg7bCrVurkSlgWcfj+LqV4NxozRaSoRSRNUbkTSGZvNRucynZk++GdaD8jPnGJgi3XAgAGYBg3g/HmrI4qI/CcqNyLpVDG/Yvzw6h42jmhPh+fhhjPY1q69c5rqhx+sjici8thUbkTSsYyuGZneZAa1Rn5FjR4eHHgCnCLOYWrVghEjIDbW6ogiIgmmciMivFL8FeYMCaXj4GLMLAk2hwOGDsXxzDMQEWF1PBGRBFG5EREA8mfJz8YeO9n9Xk/aNoHrLmD//ntiSxSDDRusjici8shUbkQkjruzO5OenUSTUf+jRi9P9mUDp/N/YurWhSFDICbG6ogiIg+lciMid2lWqBmLhuzn1aHlmFoabMbAyJE4atWEs2etjici8kAqNyJyT0GZgljfdStHx7xJyxfgqivYN28hpnhRWL3a6ngiIvdlabkJCgrCZrPdNfXs2fO+7/noo48oUKAAHh4eBAYG0rdvX27dupWMqUXSDxcnF8bVHUfb0auo85ovIf7gfOESNGgAAwboNJWIpEiWlptdu3YRHh4eN61btw6Al1566Z7j582bR//+/Rk6dCgHDx5k+vTpfP3117zzzjvJGVsk3WnwVAOWDNrP2yOq8mm5v2aOGUNs9Wpw5oyl2URE/s3ScpM1a1b8/f3jphUrVpA3b16qV69+z/Hbtm2jSpUqtGrViqCgIJ555hlatmzJzp0777uNqKgoIiMj400iknBPej/Jdx038se4obz0ElxxA6dt24kpUQxWrLA6nohInBRzzU10dDRz5syhQ4cO2Gy2e46pXLkyu3fvjiszx48fZ9WqVTz77LP3Xe/o0aPx8fGJmwIDA5Mkv0h64GR3YliNYfQY+z31Xs/KrgBwvnQFGjXCvPEGREdbHVFEBJsxxlgdAmDhwoW0atWK06dPExAQcN9xn3zyCf369cMYQ0xMDN26dWPy5Mn3HR8VFUVUVFTc68jISAIDA7ly5Qre3t6J+hlE0pPz18/TaVEbak1ZS58dd+bFlC+L89eLICjI0mwikvZERkbi4+PzSN/fKebIzfTp02nQoMEDi82mTZsYNWoUn332GXv27GHJkiWsXLmSkSNH3vc9bm5ueHt7x5tE5L/LljEby9p9R/QHY2n2sp1L7uC882diShaHZcusjici6ViKOHJz6tQp8uTJw5IlS2jcuPF9x1WtWpWKFSvy/vvvx82bM2cOXbp04dq1a9jtD+9qCWl+IvJotp/ZzhvTXmTCjLNU/P3OPPPqq9jefx/c3KwNJyJpQqo7cjNz5kyyZctGw4YNHzjuxo0bdxUYJycnAFJARxNJtyoFVmLlgF/4YGxj3q98Z55t4kRiKlWAY8esDSci6Y7l5cbhcDBz5kzatWuHs7NzvGVt27ZlwIABca8bNWrE5MmTWbBgASdOnGDdunUMHjyYRo0axZUcEbGGr4cvC1stxePDiTR5xZk/PcA5ZC8xpUrAokVWxxORdMT54UOS1vr16zl9+jQdOnS4a9np06fjHakZNGgQNpuNQYMG8fvvv5M1a1YaNWrEe++9l5yRReQ+bDYbvcr3osq4KjR9qhmjvzjJ02euQ/PmmG7dsH34Ibi7Wx1TRNK4FHHNTXLSNTciyeNq1FV6LO9CoU8W8M6WO/NuFyuCy+IlkD+/teFEJNVJddfciEja4+XmxZcvzsP/k+k8386V8xnAZf+vxJQuCfPmWR1PRNIwlRsRSTI2m40OpTowetweWgzMz8YgcL5+E1q3xtGpI9y4YXVEEUmDVG5EJMkVyVaElW+GsODDjgyvDg7APn0Gt8uVhoMHrY4nImmMyo2IJIsMLhn4vMkX5J84j+c7uBOREVwOhBFTphTMnm11PBFJQ1RuRCRZtSzWko/e30/bocVYnxucb0ZBcDCx7drC9etWxxORNEDlRkSSXb7M+fi27y5WfNKLQTUh1gZOX35FdKkS8MsvVscTkVRO5UZELOHm7MZHz02k7GdLadzZk9+9wPXIMWLLloYvvoD09SsVIpKIVG5ExFJNCjbhs3G/0mVkOb7LB05Rt6FzZ2JbtYSrV62OJyKp0GOVmzNnzvDbb7/Fvd65cyd9+vRh6tSpiRZMRNKPnD45WdZrK5s/fZu360CMDZwWfE10qeIQGmp1PBFJZR6r3LRq1YqNGzcCEBERQd26ddm5cycDBw5kxIgRiRpQRNIHFycXRj0zhlqTV9OkeybOeIPrsZPEVigPkyfrNJWIPLLHKje//PIL5cuXB2DhwoUULVqUbdu2MXfuXGbNmpWY+UQknamXrx7Txhyg9+iqfJsfnKJvQ48e3G7+Ily5YnU8EUkFHqvc3L59Gzc3N+DOgy+ff/55AAoWLEh4eHjipRORdCm7V3YWd9tI6OfDeaOejdt2cFm8hKgSReHnn62OJyIp3GOVmyJFijBlyhQ2b97MunXrqF+/PgBnz54lS5YsiRpQRNInJ7sTg2sMofHnm2ja6wlO+oDbqd+IrVQR8/HHOk0lIvf1WOVm7NixfP7559SoUYOWLVtSokQJAJYvXx53ukpEJDFUy1WNWaMP8vb7dVlaEJxiYrH16cPtJs/DpUtWxxORFMhmzOP9709sbCyRkZH4+vrGzTt58iQZMmQgW7ZsiRYwsSXkkekiknI4jIMPt03gzKi3GbvGgVssROXIjtvipVChgtXxRCSJJeT7+7GO3Ny8eZOoqKi4YnPq1Ck++ugjwsLCUnSxEZHUy26z80aVfrT8fBsv9snOMV9w+y2c2CqVMePH6zSViMR5rHLTuHFjvvzySwAuX75MhQoV+OCDD2jSpAmTJ09O1IAiIv9UIUcFvnr3ACM+bMzCwuAU68D25ptEN6wPFy5YHU9EUoDHKjd79uyhatWqACxevBg/Pz9OnTrFl19+ySeffJKoAUVE/i2TeyZmtV3KhZmf0quRM7ecwPW7tdwqXhi2brU6nohY7LHKzY0bN/Dy8gJg7dq1NGvWDLvdTsWKFTl16lSiBhQRuRebzUb38j3oPPVnWvTLyeHM4H72PI7q1XCMHgUOh9URRcQij1Vu8uXLx7Jlyzhz5gxr1qzhmWeeAeD8+fO6SFdEklUJ/xLMHfEr4ye+zNxiYI91YH9nIFH168Aff1gdT0Qs8FjlZsiQIfTr14+goCDKly9PpUqVgDtHcUqVKpWoAUVEHsbT1ZOpreYTM3sm3Zu6ctMZ3NZt5FaxQvDjj1bHE5Fk9ti3gkdERBAeHk6JEiWw2+90pJ07d+Lt7U3BggUTNWRi0q3gImnbwT8OMvCTxrw75QiF/wSH3YYZNgyndwaCk5PV8UTkMSXk+/uxy83f/n46eI4cOf7LapKNyo1I2nfz9k36f/Mqpd6dTvDeO/NuVa+C+4LF4O9vbTgReSxJ/js3DoeDESNG4OPjQ65cuciVKxeZMmVi5MiROHQRn4hYzMPFg49f/IIMc7+m24vuXHcB9x+23jlNtWGD1fFEJIk9VrkZOHAgkyZNYsyYMYSEhBASEsKoUaOYOHEigwcPTuyMIiKPpXmR5rw19VeCBxVhfzZw//Myjrp1iB08CGJjrY4nIknksU5LBQQEMGXKlLingf/tm2++oUePHvz++++JFjCx6bSUSPoTHRvN4BVvkG/EJDrvuTPvZpXyeCxcCgEB1oYTkUeS5KelLl68eM+LhgsWLMjFixcfZ5UiIknG1cmVsY0n4j9vOV1ezshVV/DYuvPOaao1a6yOJyKJ7LHKTYkSJZg0adJd8ydNmkTx4sX/cygRkaTQqEAjBk85SNdhZQj1A/eLkVC/PrffehNiYqyOJyKJ5LFOS/3www80bNiQnDlzxv3Gzfbt2zlz5gyrVq2KezRDSqTTUiIS44jh3bWDyDZkLD123Zl3o3xpMixeBoGBlmYTkXtL8tNS1atX5/DhwzRt2pTLly9z+fJlmjVrxq+//spXX331WKFFRJKLs92ZYfXHkH/BOjq/4sMVN8iwcw+3ihXCrFhhdTwR+Y/+8+/c/NPevXspXbo0sSn4LgQduRGRf4q4FsFbU16g94RtlA2/My+6b29cx44HFxdrw4lInCQ/ciMiklb4e/oz643NbPhqOJ9UsAHg+uEnXK9YBk6etDaciDwWlRsRSffsNjtv1x5C6YU/0ql9Fi65Q8Y9+7lVvDBm6VKr44lIAqnciIj85emcTzN2Uhhvjq3NT0+C+9Wb2Jo1I6pXd4iKsjqeiDyiBF1z06xZswcuv3z5Mj/88IOuuRGRVM0Yw8QtHxDT/21e33bnkTLXihXEc9lKyJPH4nQi6VOSXXPj4+PzwClXrly0bdv2P4UXEbGazWajd9V+VFu0g06d/bjgAZ77DxFVvAiORQutjiciD5God0ulBjpyIyIJceXWFQbMeoVWo1bw9Jk78252bo/HJ5+Bu7u14UTSkVRzt1RQUBA2m+2uqWfPnvd9z+XLl+nZsyfZs2fHzc2N/Pnzs2rVqmRMLSLpiY+7D592Xc7BRZ8xrpoTAB7TZnK1TDE4csTidCJyL5aWm127dhEeHh43rVu3DoCXXnrpnuOjo6OpW7cuJ0+eZPHixYSFhTFt2jSefPLJ5IwtIumMzWajc4XuNFgYQqceOfgjA3gdOEpUiaI45s61Op6I/IuzlRvPmjVrvNdjxowhb968VK9e/Z7jZ8yYwcWLF9m2bRsuf/24VlBQUFLHFBEBoJhfMT7+8BCDynag8fCF1DgVDa+8wo3135Hhs2ng4WF1RBEhBd0KHh0dzZw5c+jQoQM2m+2eY5YvX06lSpXo2bMnfn5+FC1alFGjRj3w7qyoqCgiIyPjTSIijyuja0Y+bP81v/1vJmNquuAAMsyay9WSheHQIavjiQgpqNwsW7aMy5cvExwcfN8xx48fZ/HixcTGxrJq1SoGDx7MBx98wLvvvnvf94wePTreHV2BeiieiCSCV8oE03Thfrr3zkNERvA6fJLoUsWJmTXD6mgi6V6KuVuqXr16uLq68u233953TP78+bl16xYnTpzAyenOhX0TJkzg/fffJzw8/J7viYqKIuofP74VGRlJYGCg7pYSkURxK+YWwxd0p/aQWdQ5cWfetVYv4jl1FmTMaGk2kbQk1dwt9bdTp06xfv16OnXq9MBx2bNnJ3/+/HHFBqBQoUJEREQQHR19z/e4ubnh7e0dbxIRSSzuzu6MfmUmV775mvfquBFrA895i4ksURB++cXqeCLpUoooNzNnziRbtmw0bNjwgeOqVKnC0aNHcTgccfMOHz5M9uzZcXV1TeqYIiL39UKx5rRaeJCebxTkrCd4H/uN6LKluD11CqSMA+Qi6Ybl5cbhcDBz5kzatWuHs3P8m7fatm3LgAED4l53796dixcv8tprr3H48GFWrlzJqFGjHvi7OCIiySW3b24+GbOXqdO7szovuEbF4NK1O5HNm8DVq1bHE0k3LC8369ev5/Tp03To0OGuZadPn453LU1gYCBr1qxh165dFC9enN69e/Paa6/Rv3//5IwsInJfrk6uDGv+GY6V3zKifgZibOC9eDmRxQrA3r1WxxNJF1LMBcXJRY9fEJHk8nvk74wa25D+k/YSGAm3XZwwH07AtcercJ+fvBCRe0t1FxSLiKRFT3o/yccjf2bu7H58mx9cbsfi2us1rjRpAPrNLZEko3IjIpKEnO3O9G/yPhm/W8+w57y4bQef5WuILJIP8/PPVscTSZNUbkREkkGtPLXpseAo/QaX56QPeP/2BzGVKnDzw/d1N5VIIlO5ERFJJtkyZuPDodtZNn8IywqCS4wDj9ff4nLD2nDpktXxRNIMlRsRkWRkt9np02A42dZsYWhTX6LtkOm7jVwpkg+zY4fV8UTSBJUbERELVM5ZhdfmHmXAiGoc8wWf8IvEVqnEjbHv6jSVyH+kciMiYpHMHpkZ/84m1i8ay/+K2HCONWToP5iLz1SDixetjieSaqnciIhYyGaz0bX2W+Reu4uhL2XllhNkXr+FyEJ5cGzdYnU8kVRJ5UZEJAUoHVCGN746yvAx9TmcGbzPX8FUq8a1kYPhH8/TE5GHU7kREUkhvN28GfXGKn5aNokFxZ1wchg8h7zLhVqV4I8/rI4nkmqo3IiIpCA2m422VXtSdG0IQ1pl56YzZPlhJ5GF8hK7aaPV8URSBZUbEZEUqKhfMfrPOsqYD5py8AnwvnAVatUicvBbEBtrdTyRFE3lRkQkhcrgkoHhvZewb8V05pZ2xsmA97vv82e1snDunNXxRFIslRsRkRSuRYUOlF97gCHtcnLdBZ7YFkpk4bzcXrfG6mgiKZLKjYhIKvBUlqcY+MVhPp7Yml+ygvfF6zjVq8/lt1/TaSqRf1G5ERFJJdyc3Xin6xyOr57P7HKu2A1kGvcJ5yuVgLNnrY4nkmKo3IiIpDLPl36ZGmsPM7RzPq66QrZdv3K1cD6iv1thdTSRFEHlRkQkFcqVKReDJh/g8887E+oHXldu4vpsIy707QYxMVbHE7GUyo2ISCrl4uRCv+CpnN/wDTMruQOQ5aPPOVe+CPz2m8XpRKyjciMikso9U+R56q05xvAeRYh0Bb+Qw1wtnI9b3yyxOpqIJVRuRETSgACvAAZN3Mvsma/xcwB4XY3CvckL/NGzPdy+bXU8kWSlciMikkY42Z14tdVH3Ph+LdOfzghA1s9mca5MAczJk9aGE0lGKjciImlMtQJ1eX71CUa+VopL7uC3/wTXixbg+qJ5VkcTSRYqNyIiaVDWjFkZ+OHPLJozgJ1Pguf1aDI2b825Ti0hOtrqeCJJSuVGRCSNstvsdHlhFPy4mak1vAHwm76AiJL5MMeOWZxOJOmo3IiIpHHl8zzNS6tOMvqtSlzwAP+DZ7hRrCBX5860OppIklC5ERFJB3w9fOk/Zisrvx7Jtpw2Mt6MweuVDpxt2wxu3bI6nkiiUrkREUknbDYbbRsNIsOWHXxe1xeAgK+WElE8D47DYRanE0k8KjciIulMycBytPr2FOMG1eKPDOB/JJxbJYpwedYUq6OJJAqVGxGRdMjLzYs3R6xn49IJbA6yk+FWLJnad+e3ls/BzZtWxxP5T1RuRETSKZvNRvNn+pJlWwifN8iKA8ixYCURRYKIOfCL1fFEHpvKjYhIOlc4e3HafnOKj0c8S0RG8D9xnujSJbg49WOro4k8FpUbERHBw8WDvoNXsmPFFDblcSJDlIPMXftw+oU6cP261fFEEkTlRkRE4jSu0ZXAnw4wpVF2Ym2Qc8kGIgrn5Pa+UKujiTwylRsREYknb9b8tF9ygsljXuCsJ/ifvkhs2TL8MXEsGGN1PJGHUrkREZG7uDm70eutxexbM5vvn3LG/baDrL37c6pRNbh2zep4Ig+kciMiIvdVv3Jb8u04wpQXchFjg1wrtxBRMAe3ft5hdTSR+1K5ERGRB8rpG0SnhUeZMaENZ7zB//cr2CpVIuKD4TpNJSmSpeUmKCgIm81219SzZ8+HvnfBggXYbDaaNGmS9EFFRNI5Z7szXfp8ybH1i1hXyBW3GIN/v2GcqF8RIiOtjicSj6XlZteuXYSHh8dN69atA+Cll1564PtOnjxJv379qFq1anLEFBGRv9Qo9yLFfjrB5y3ycdsOudfu5FyBHNzYscXqaCJxLC03WbNmxd/fP25asWIFefPmpXr16vd9T2xsLK1bt2b48OHkyZMnGdOKiAiAv3cAneYdYs6krpzMBH4RV3F+uhq/jRqg01SSIqSYa26io6OZM2cOHTp0wGaz3XfciBEjyJYtGx07dnyk9UZFRREZGRlvEhGR/8bJ7kT77lMI/2Elq4u44xpjyDFwDMdrl8ZcumR1PEnnUky5WbZsGZcvXyY4OPi+Y7Zs2cL06dOZNm3aI6939OjR+Pj4xE2BgYGJkFZERAAqFX+WsjtOM7VNYaLtkGdjKOcL5ODq5g1WR5N0LMWUm+nTp9OgQQMCAgLuufzq1au0adOGadOm8cQTTzzyegcMGMCVK1fipjNnziRWZBERAZ7ImJXOs39h8dQ+HPcFvz9u4FazDqeG9dVpKrGEzRjr/+SdOnWKPHnysGTJEho3bnzPMaGhoZQqVQonJ6e4eQ6HAwC73U5YWBh58+Z96LYiIyPx8fHhypUreHt7J84HEBERAPYc3EhEy+d4du8NAI5VKUzub37AnuXR/6dU5F4S8v2dIo7czJw5k2zZstGwYcP7jilYsCD79+8nNDQ0bnr++eepWbMmoaGhOt0kIpIClC5Ukyrbf2dap9LccoK8Ww/wZ4FALn+/yupoko5YXm4cDgczZ86kXbt2ODs7x1vWtm1bBgwYAIC7uztFixaNN2XKlAkvLy+KFi2Kq6urFfFFRORffDwy0Wnqz6yYPZAjWWxku3ALz7oNOT6gG/x1xF0kKVlebtavX8/p06fp0KHDXctOnz5NeHi4BalEROS/sNlsvNj6XaJ2bGVFGW+cHZBnzOccqVyQ2HMRVseTNC5FXHOTnHTNjYhI8roWdZVFfevx8rTteMTAn5lcYcHXPFGvidXRJBVJddfciIhI2uXp5kX7z7axbv67hGW18cTlaHwbNOXIG8E6TSVJQuVGRESSxfMvDoSdu1hewRcnA09NmM2Rcnm5ffY3q6NJGqNyIyIiyaZAUBnqbv6dL/vU5LoLPLXnJFcK5+Hc8vlWR5M0ROVGRESSlYeLB20//J4tiydwwM/OE1duk7VJKw71ehliY62OJ2mAyo2IiFii3vN9ybB7P988nRW7gYKffs3h0kFEnTlpdTRJ5VRuRETEMkFPFqbBpt+Y+/azXHOB/Pt+43qRp/j9fzOtjiapmMqNiIhYytXJldZjVvLz8in8kt2JzFdjyP5SB37t0hRiYqyOJ6mQyo2IiKQINep3JVPIQZbV8MduoMi0ZRwuEcjNE0esjiapjMqNiIikGDn8nuK5DWf4ekgzIl0h/4EIbhUrxOn5U6yOJqmIyo2IiKQoznZnWgz/H798N5t9Tzrjez2WnK26s69dA0x0tNXxJBVQuRERkRSpcq22+O09xrK6gQAU/3I1R4o/ybUjByxOJimdyo2IiKRYflly8vyakyx5tzWX3SF/2J/ElCzGsVkfWR1NUjCVGxERSdHsNjvNBs7h6LqFhAa6kumGg7zt+xLasiYmKsrqeJICqdyIiEiqUPbpl8i5/xTLns0DQMkFmzhaJDtXDoZaG0xSHJUbERFJNTL7+NN4xVG+fb8zFz3gqWOXsJUuTdjno6yOJimIyo2IiKQqNpuNRv2m8tsPK9gT5Ib3LUOBbgPZ80JlHDdvWB1PUgCVGxERSZWKl2tIvv2/802TQgCUXrKd44Wyc2HfDouTidVUbkREJNXy9szC80t+ZfUnvfkjA+Q7FYlr+UocmDjE6mhiIZUbERFJ1Ww2G/Vf/ZgL29bzc94MeEUZCvceyc+NyhB7/ZrV8cQCKjciIpImFCxRm0L7zvLtSyVwAGVX7OFEIX/O7f7R6miSzFRuREQkzciYwYdGC0PZOOVtzme0ke/MdTwrV2fv+DetjibJSOVGRETSnNpdx3B1x4/sLOBJxmgo8eZ4dtUvxu2rV6yOJslA5UZERNKkvEWepvjeCFa2Lk+sDcqt+YUzBbPz+/a1VkeTJKZyIyIiaZa7W0YaztnB1pnDifCykefsTXyr12P3e73AGKvjSRJRuRERkTSvWrshRO/eyU+FfchwG8oM+pQddQpx6/KfVkeTJKByIyIi6ULOp8pSJvQcq9tXJdYGFb4PI7xgDk7+uNzqaJLIVG5ERCTdcHFxo/6MH9k1ZxxnfezkPheFf+3G7BjSUaep0hCVGxERSXcqtnoTe+hefiqeGfcYqDByBjuq5+X6n+FWR5NEoHIjIiLpkn9QUcrtjmBtt7rctkOFzSf4s3Aujm5YZHU0+Y9UbkREJN1ycnbhmclr2b9oEr9lciLXH7cJrNecrW+3wjgcVseTx6RyIyIi6V7pZj1x2/cr20tnwy0Wqoybz87KuYg8d9rqaPIYVG5ERESArIEFqLDrLBt6P0+0E1TY8RuXi+Tl4KovrY4mCaRyIyIi8he73YnaH39D2LLpnM7sTM4LMeR9vh0/9Gmq01SpiMqNiIjIvxR7rgPevxxhe/kAXGOh+sfL2Fn+SS79fszqaPIIVG5ERETuIVP2ICpuP8MP/V4iygkq7I7gerEC7Fv6udXR5CFUbkRERO7DZrdT/f2FnFg1j5NZXchxKZZCL3bj+x4NcMTGWB1P7kPlRkRE5CEKPtOSLL+eYHuVXLg4oNbk1ewqk53zJw9YHU3uQeVGRETkEXhlfZKKPx5n66B23HSGCnv/JKZkMXZ//ZHV0eRfLC03QUFB2Gy2u6aePXvec/y0adOoWrUqvr6++Pr6UqdOHXbu3JnMqUVEJL2y2e1UGTmLs+uWcMLPlYArDkq07Mu6jjWJiYm2Op78xdJys2vXLsLDw+OmdevWAfDSSy/dc/ymTZto2bIlGzduZPv27QQGBvLMM8/w+++/J2dsERFJ5/LWaIrfwTNsr5kPZwN1Z2wipKQfZ4+FWh1NAJsxKecxqH369GHFihUcOXIEm8320PGxsbH4+voyadIk2rZt+0jbiIyMxMfHhytXruDt7f1fI4uISHpmDDve7U6xkZ+T4Tac87JxfNK7VGr7jtXJ0pyEfH+nmGtuoqOjmTNnDh06dHikYgNw48YNbt++TebMme87JioqisjIyHiTiIhIorDZqDB4Cn9sWsWx7O74XTWUDx7ImjaViY6+aXW6dCvFlJtly5Zx+fJlgoODH/k9b7/9NgEBAdSpU+e+Y0aPHo2Pj0/cFBgYmAhpRURE/l+uyg3IcegsPz1TGCcD9eZsZ28JP04d/MnqaOlSiik306dPp0GDBgQEBDzS+DFjxrBgwQKWLl2Ku7v7fccNGDCAK1euxE1nzpxJrMgiIiJx3Lx9qbjmV3aP7cM1Vyh36Coe5Svzw7TBVkdLd1JEuTl16hTr16+nU6dOjzR+/PjxjBkzhrVr11K8ePEHjnVzc8Pb2zveJCIiklTKvPUhkVs2cCRHBrJdM1Tt8i7ftSjDzZtXrY6WbqSIcjNz5kyyZctGw4YNHzp23LhxjBw5ktWrV1O2bNlkSCciIpIwAeVqEXQwnB3PlcIONFi4hwPFs3N03yaro6ULlpcbh8PBzJkzadeuHc7OzvGWtW3blgEDBsS9Hjt2LIMHD2bGjBkEBQURERFBREQE165dS+7YIiIiD+Ti6U2Fb/ew9+MBXHWzUebodTJVqsn6T9+wOlqaZ3m5Wb9+PadPn6ZDhw53LTt9+jTh4eFxrydPnkx0dDQvvvgi2bNnj5vGjx+fnJFFREQeWYneo7j50xYO5/LiiRtQp9cEVjYryrXrl6yOlmalqN+5SQ76nRsREbFC7M0b7Gldi3JLdwAQktsd90XLKFSmnsXJUodU+Ts3IiIiaZmTRwbKLfmJA5NHcMXdRqkTt/CrWp/vPuhOOjvOkORUbkRERJJR4W6Did29i8N5fMh8Exr0m8LKRgW4HHne6mhphsqNiIhIMstcuAz5DkSwu0U1AJ5beYSTxXKyd/sya4OlESo3IiIiFrC7uVNmwQ8cnvE+lzPYKXk6ily1mrJ8dLBOU/1HKjciIiIWyt++H7Y9IYTlz0ymW/D8O7NZUT83f1763epoqZbKjYiIiMV8ChQn//5wQto+A0Cjtac4Wzw3u36Yb3Gy1EnlRkREJAWwubpSavYajs+ZyKWMdor/dpsC9VqxZFgLYh2xVsdLVVRuREREUpA8rXvhuu8AYYWz4R0FzYYvZFWdXET8ccLqaKmGyo2IiEgKkzFPAQrs/Z19nRrhsEGjjb9zoUR+tq6bYXW0VEHlRkREJCVydqb4tOX8tvALLng5UyQ8hhINO7JwwPPEOGKsTpeiqdyIiIikYDlf7EiGX8IIKx6A521oPuZbvqv+JGfCw6yOlmKp3IiIiKRwHjnzUGDPaX7t8dKd01RbznOtVGG+XznJ6mgpksqNiIhIauDkRJFPFxKxbA5/ertQ6JyDik1eZd4bzxAVE2V1uhRF5UZERCQVCXi+Nd4HjxFWOhcZYqDVhHWsqxrA8dP7rI6WYqjciIiIpDKuAYEU2HWcQ33bEGuD5366yO2ypfhuyTiro6UIKjciIiKpkd1OwQlfcmHVYs77ulLgDwc1WrzNl72qcSP6utXpLKVyIyIikoplq/8CmQ+d4nD5vHjEQNtPN7OpcgCHju+0OpplVG5ERERSOeds/uTffpgjb3cmxg7P7o7EuXxFvpk/LF0+YVzlRkREJC2w23lqzFQi167gXBZ38l0w1G8znNldK3D1VqTV6ZKVyo2IiEgakrl2Q7KGneFwlUK4xULwtF1sqfQk+w5vtjpaslG5ERERSWPsWZ4g/+ZfOT6kN9FO0CD0Gl4Vq7Fo1lvp4jSVyo2IiEhaZLORZ/jH3Ni4lnNZPch9CRp3fJ8ZwSW4dOOi1emSlMqNiIhIGpapal2yhf3OkRrFcXVAxy/3s6tCDnbtX2N1tCSjciMiIpLG2Xx9eer7UE699yZRTvDMLzfJVrU+cz7vhcM4rI6X6FRuRERE0gObjVzvjCN68yYi/DzJdQVa9PiUGa0L88fVc1anS1QqNyIiIumIV6Xq+IX9xrG6ZXFxQKf5YeytkIute76xOlqiUbkRERFJZ2w+PuRds5Pf3h9ClLONOgejyFWzCTMmdiDWEWt1vP9M5UZERCQ9stnI0W84sT9tJ/xJH3JEQtvXZjKjxVOcvfKb1en+E5UbERGRdCxDmQpkP/QbxxtWwdlA58UnCCufh+93LLA62mNTuREREUnvPD3J8+1mIj4ZxU0XGzUP36ZgnZZ8Pr4lt2NvW50uwVRuREREBGw2/F8dgH3Xz4Tn9CXgGnR6awGzXsjDyQvHrE6XICo3IiIiEsetRGmyHzjDyaY1cTLQ+ZvfOFWhIKt+nGF1tEemciMiIiLxZcxI0JLv+WPyB9xws1P9WAxlnu3IpPeacCvmltXpHkrlRkRERO4pa7fXcdkdSnjuJ/C7Dj0GfcNXjYM4cu6g1dEeSOVGRERE7sulSDGy/3qa0y0aYAc6rzrHuYpFWbp+ktXR7kvlRkRERB7Mw4OcC1Zxcfqn3HB34umTDp5u/CofDq3H9ejrVqe7i8qNiIiIPJLMHXrgGrqfs/n8yXoD+o5Yy/zncvHL7yFWR4tH5UZEREQemXOBQgTsP8FvbRoD0GndBa5WLsP8lWMxxlic7g5Ly01QUBA2m+2uqWfPnvd9z6JFiyhYsCDu7u4UK1aMVatWJWNiERERwd2dHF8u48qc6VzL4Eyl04ZnXurP+HeqExkVaXU6a8vNrl27CA8Pj5vWrVsHwEsvvXTP8du2baNly5Z07NiRkJAQmjRpQpMmTfjll1+SM7aIiIgAPq07kGHfQcIL5SDLTXhzzGYW1c/JnpM/WZrLZlLKMSSgT58+rFixgiNHjmCz2e5a3qJFC65fv86KFSvi5lWsWJGSJUsyZcqUR9pGZGQkPj4+XLlyBW9v70TLLiIikm5FR3O2R1sCpn8NwO5AZwrtDyeDzxOJtomEfH+nmGtuoqOjmTNnDh06dLhnsQHYvn07derUiTevXr16bN++/b7rjYqKIjIyMt4kIiIiicjVlYAvFnD166+4ltEF3wrVE7XYJFSKKTfLli3j8uXLBAcH33dMREQEfn5+8eb5+fkRERFx3/eMHj0aHx+fuCkwMDCxIouIiMg/eDV/hYy/HiHPl99amiPFlJvp06fToEEDAgICEnW9AwYM4MqVK3HTmTNnEnX9IiIi8v9suXKBh4elGZwt3fpfTp06xfr161myZMkDx/n7+3Pu3Ll4886dO4e/v/993+Pm5oabm1ui5BQREZGUL0UcuZk5cybZsmWjYcOGDxxXqVIlNmzYEG/eunXrqFSpUlLGExERkVTE8nLjcDiYOXMm7dq1w9k5/oGktm3bMmDAgLjXr732GqtXr+aDDz7g0KFDDBs2jJ9//plevXold2wRERFJoSwvN+vXr+f06dN06NDhrmWnT58mPDw87nXlypWZN28eU6dOpUSJEixevJhly5ZRtGjR5IwsIiIiKViK+p2b5KDfuREREUl9UuXv3IiIiIgkBpUbERERSVNUbkRERCRNUbkRERGRNEXlRkRERNIUlRsRERFJU1RuREREJE1RuREREZE0JUU8ODM5/f2bhZGRkRYnERERkUf19/f2o/z2cLorN1evXgUgMDDQ4iQiIiKSUFevXsXHx+eBY9Ld4xccDgdnz57Fy8sLm82WqOuOjIwkMDCQM2fO6NEOSUj7OXloPycP7efko32dPJJqPxtjuHr1KgEBAdjtD76qJt0dubHb7eTIkSNJt+Ht7a3/cJKB9nPy0H5OHtrPyUf7OnkkxX5+2BGbv+mCYhEREUlTVG5EREQkTVG5SURubm4MHToUNzc3q6OkadrPyUP7OXloPycf7evkkRL2c7q7oFhERETSNh25ERERkTRF5UZERETSFJUbERERSVNUbkRERCRNUblJoE8//ZSgoCDc3d2pUKECO3fufOD4RYsWUbBgQdzd3SlWrBirVq1KpqSpW0L287Rp06hatSq+vr74+vpSp06dh/57kTsS+uf5bwsWLMBms9GkSZOkDZhGJHQ/X758mZ49e5I9e3bc3NzInz+//u54BAndzx999BEFChTAw8ODwMBA+vbty61bt5Ipber0448/0qhRIwICArDZbCxbtuyh79m0aROlS5fGzc2NfPnyMWvWrCTPiZFHtmDBAuPq6mpmzJhhfv31V9O5c2eTKVMmc+7cuXuO37p1q3FycjLjxo0zBw4cMIMGDTIuLi5m//79yZw8dUnofm7VqpX59NNPTUhIiDl48KAJDg42Pj4+5rfffkvm5KlLQvfz306cOGGefPJJU7VqVdO4cePkCZuKJXQ/R0VFmbJly5pnn33WbNmyxZw4ccJs2rTJhIaGJnPy1CWh+3nu3LnGzc3NzJ0715w4ccKsWbPGZM+e3fTt2zeZk6cuq1atMgMHDjRLliwxgFm6dOkDxx8/ftxkyJDBvP766+bAgQNm4sSJxsnJyaxevTpJc6rcJED58uVNz549417HxsaagIAAM3r06HuOb968uWnYsGG8eRUqVDBdu3ZN0pypXUL387/FxMQYLy8vM3v27KSKmCY8zn6OiYkxlStXNl988YVp166dys0jSOh+njx5ssmTJ4+Jjo5OrohpQkL3c8+ePU2tWrXizXv99ddNlSpVkjRnWvIo5eatt94yRYoUiTevRYsWpl69ekmYzBidlnpE0dHR7N69mzp16sTNs9vt1KlTh+3bt9/zPdu3b483HqBevXr3HS+Pt5//7caNG9y+fZvMmTMnVcxU73H384gRI8iWLRsdO3ZMjpip3uPs5+XLl1OpUiV69uyJn58fRYsWZdSoUcTGxiZX7FTncfZz5cqV2b17d9ypq+PHj7Nq1SqeffbZZMmcXlj1PZjuHpz5uP78809iY2Px8/OLN9/Pz49Dhw7d8z0RERH3HB8REZFkOVO7x9nP//b2228TEBBw139Q8v8eZz9v2bKF6dOnExoamgwJ04bH2c/Hjx/n+++/p3Xr1qxatYqjR4/So0cPbt++zdChQ5MjdqrzOPu5VatW/Pnnnzz99NMYY4iJiaFbt2688847yRE53bjf92BkZCQ3b97Ew8MjSbarIzeSpowZM4YFCxawdOlS3N3drY6TZly9epU2bdowbdo0nnjiCavjpGkOh4Ns2bIxdepUypQpQ4sWLRg4cCBTpkyxOlqasmnTJkaNGsVnn33Gnj17WLJkCStXrmTkyJFWR5NEoCM3j+iJJ57AycmJc+fOxZt/7tw5/P397/kef3//BI2Xx9vPfxs/fjxjxoxh/fr1FC9ePCljpnoJ3c/Hjh3j5MmTNGrUKG6ew+EAwNnZmbCwMPLmzZu0oVOhx/nznD17dlxcXHBycoqbV6hQISIiIoiOjsbV1TVJM6dGj7OfBw8eTJs2bejUqRMAxYoV4/r163Tp0oWBAwdit+v//RPD/b4Hvb29k+yoDejIzSNzdXWlTJkybNiwIW6ew+Fgw4YNVKpU6Z7vqVSpUrzxAOvWrbvveHm8/Qwwbtw4Ro4cyerVqylbtmxyRE3VErqfCxYsyP79+wkNDY2bnn/+eWrWrEloaCiBgYHJGT/VeJw/z1WqVOHo0aNx5RHg8OHDZM+eXcXmPh5nP9+4ceOuAvN3oTR65GKisex7MEkvV05jFixYYNzc3MysWbPMgQMHTJcuXUymTJlMRESEMcaYNm3amP79+8eN37p1q3F2djbjx483Bw8eNEOHDtWt4I8goft5zJgxxtXV1SxevNiEh4fHTVevXrXqI6QKCd3P/6a7pR5NQvfz6dOnjZeXl+nVq5cJCwszK1asMNmyZTPvvvuuVR8hVUjofh46dKjx8vIy8+fPN8ePHzdr1641efPmNc2bN7fqI6QKV69eNSEhISYkJMQAZsKECSYkJMScOnXKGGNM//79TZs2beLG/30r+JtvvmkOHjxoPv30U90KnhJNnDjR5MyZ07i6upry5cubn376KW5Z9erVTbt27eKNX7hwocmfP79xdXU1RYoUMStXrkzmxKlTQvZzrly5DHDXNHTo0OQPnsok9M/zP6ncPLqE7udt27aZChUqGDc3N5MnTx7z3nvvmZiYmGROnfokZD/fvn3bDBs2zOTNm9e4u7ubwMBA06NHD3Pp0qXkD56KbNy48Z5/3/69b9u1a2eqV69+13tKlixpXF1dTZ48eczMmTOTPKfNGB1/ExERkbRD19yIiIhImqJyIyIiImmKyo2IiIikKSo3IiIikqao3IiIiEiaonIjIiIiaYrKjYiIiKQpKjciIiKSpqjciIikM5s2bcJms3H58mWro4gkCZUbEQv88ccfdO/enZw5c+Lm5oa/vz/16tVj69atcWNsNhvLli2zLmQC/P1lea8pIiLC6nh3CQ8Pp1WrVuTPnx+73U6fPn3uOW7RokUULFgQd3d3ihUrxqpVq+ItN8YwZMgQsmfPjoeHB3Xq1OHIkSPJ8AlE5EFUbkQs8MILLxASEsLs2bM5fPgwy5cvp0aNGly4cMHqaP9JWFgY4eHh8aZs2bIl2faio6Mf631RUVFkzZqVQYMGUaJEiXuO2bZtGy1btqRjx46EhITQpEkTmjRpwi+//BI3Zty4cXzyySdMmTKFHTt2kDFjRurVq8etW7ceK5eIJJIkf3qViMRz6dIlA5hNmzbdd8y/HwaaK1euuGXLli0zpUqVMm5ubiZ37txm2LBh5vbt23HLAfPZZ5+Z+vXrG3d3d5M7d26zaNGiuOVRUVGmZ8+ext/f37i5uZmcOXOaUaNG/afP9PfD9O730ME1a9YYNze3u5b37t3b1KxZM+715s2bzdNPP23c3d1Njhw5zKuvvmquXbsWb7+MGDHCtGnTxnh5eZl27dqZmjVrmp49e8Zb7/nz542Li4tZv379Q7NXr17dvPbaa3fNb968uWnYsGG8eRUqVDBdu3Y1xhjjcDiMv7+/ef/99+OWX7582bi5uZn58+ffd3uxsbFm1KhRJigoyLi7u5vixYvH+/fz975csWKFKVasmHFzczMVKlQw+/fvj7eexYsXm8KFCxtXV1eTK1cuM378+HjLb926Zd566y2TI0cO4+rqavLmzWu++OKLeNtYv369KVOmjPHw8DCVKlUyhw4dint/aGioqVGjhvH09DReXl6mdOnSZteuXQ/ZmyIpg8qNSDK7ffu28fT0NH369DG3bt2655jz588bwMycOdOEh4eb8+fPG2OM+fHHH423t7eZNWuWOXbsmFm7dq0JCgoyw4YNi3svYLJkyWKmTZtmwsLCzKBBg4yTk5M5cOCAMcaY999/3wQGBpoff/zRnDx50mzevNnMmzfvP32mh5WbmJgY4+fnF/fleq95R48eNRkzZjQffvihOXz4sNm6daspVaqUCQ4OjntPrly5jLe3txk/frw5evSoOXr0qJk7d67x9fWNty8nTJhggoKCjMPheGj2+5WbwMBA8+GHH8abN2TIEFO8eHFjjDHHjh0zgAkJCYk3plq1aqZ379733d67775rChYsaFavXm2OHTtmZs6cadzc3OLK7t/7slChQmbt2rVm37595rnnnjNBQUEmOjraGGPMzz//bOx2uxkxYoQJCwszM2fONB4eHvGetty8eXMTGBholixZYo4dO2bWr19vFixYEG8bFSpUMJs2bTK//vqrqVq1qqlcuXLc+4sUKWJeeeUVc/DgQXP48GGzcOFCExoa+tD9KZISqNyIWGDx4sXG19fXuLu7m8qVK5sBAwaYvXv3xhsDmKVLl8abV7t27buOsnz11Vcme/bs8d7XrVu3eGMqVKhgunfvbowx5tVXXzW1atV6pC/+R/X3l2XGjBnjTYULF44b89prr5latWrFvf730ZyOHTuaLl26xFvv5s2bjd1uNzdv3jTG3Ck3TZo0iTfm5s2bxtfX13z99ddx84oXLx6v8D3I/cqNi4vLXaXv008/NdmyZTPGGLN161YDmLNnz8Yb89JLL5nmzZvfc1u3bt0yGTJkMNu2bYs3v2PHjqZly5bGmP/fl38XEWOMuXDhgvHw8Ij7jK1atTJ169aNt44333wzbn+HhYUZwKxbt+6eOf555OZvK1euNEDcvvby8jKzZs265/tFUjpdcyNigRdeeIGzZ8+yfPly6tevz6ZNmyhdujSzZs164Pv27t3LiBEj8PT0jJs6d+5MeHg4N27ciBtXqVKleO+rVKkSBw8eBCA4OJjQ0FAKFChA7969Wbt27X23t3nz5njbmjt37gPzbd68mdDQ0Ljpnxfgtm7dmk2bNnH27FkA5s6dS8OGDcmUKVPcZ5s1a1a87dWrVw+Hw8GJEyfi1lO2bNl423R3d6dNmzbMmDEDgD179vDLL78QHBz8wKxWOHr0KDdu3KBu3brxPueXX37JsWPH4o3957/DzJkzU6BAgbh/hwcPHqRKlSrxxlepUoUjR44QGxtLaGgoTk5OVK9e/YF5ihcvHvfP2bNnB+D8+fMAvP7663Tq1Ik6deowZsyYu/KJpGTOVgcQSa/c3d2pW7cudevWZfDgwXTq1ImhQ4c+8Ev52rVrDB8+nGbNmt1zfY+idOnSnDhxgu+++47169fTvHlz6tSpw+LFi+8aW7ZsWUJDQ+Ne+/n5PXDduXPnjisr/1auXDny5s3LggUL6N69O0uXLo1X5q5du0bXrl3p3bv3Xe/NmTNn3D9nzJjxruWdOnWiZMmS/Pbbb8ycOZNatWqRK1euB2Z9GH9/f86dOxdv3rlz5/D3949b/ve8v4vB369Llix5z3Veu3YNgJUrV/Lkk0/GW+bm5vaf8v6Th4fHI41zcXGJ+2ebzQaAw+EAYNiwYbRq1YqVK1fy3XffMXToUBYsWEDTpk0TLadIUlG5EUkhChcuHO/WbxcXF2JjY+ONKV26NGFhYeTLl++B6/rpp59o27ZtvNelSpWKe+3t7U2LFi1o0aIFL774IvXr1+fixYtkzpw53no8PDweuq2EaN26NXPnziVHjhzY7XYaNmwYt6x06dIcOHDgsbZXrFgxypYty7Rp05g3bx6TJk36z1krVarEhg0b4t0mvm7durgjKrlz58bf358NGzbElZnIyEh27NhB9+7d77nOwoUL4+bmxunTpx96VOWnn36KK3WXLl3i8OHDFCpUCIBChQrF+9kAgK1bt5I/f36cnJwoVqwYDoeDH374gTp16jzOxwcgf/785M+fn759+9KyZUtmzpypciOpg9XnxUTSmz///NPUrFnTfPXVV2bv3r3m+PHjZuHChcbPz8906NAhbtxTTz1lunfvbsLDw83FixeNMcasXr3aODs7m2HDhplffvnFHDhwwMyfP98MHDgw7n2AeeKJJ8z06dNNWFiYGTJkiLHb7ebXX381xhjzwQcfmHnz5pmDBw+asLAw07FjR+Pv729iY2Mf+zP9fQ1HWFiYCQ8Pjzf9fRGsMcYcOXLEAKZ48eKmY8eO8daxd+9e4+HhYXr27GlCQkLM4cOHzbJly+LdCZUrV667LvL929SpU42rq6vx9fWNu27kQUJCQkxISIgpU6aMadWqlQkJCYnbR8bcuabG2dnZjB8/3hw8eNAMHTrUuLi4xLtracyYMSZTpkzmm2++Mfv27TONGzc2uXPnfuD2Bw4caLJkyWJmzZpljh49anbv3m0++eSTuOtb/t6XRYoUMevXrzf79+83zz//vMmZM6eJiooyxhize/fueBcUz5o1664LioODg01gYKBZunSpOX78uNm4cWPcNTv3ugA8JCTEAObEiRPmxo0bpmfPnmbjxo3m5MmTZsuWLSZv3rzmrbfeeuh+FUkJVG5EktmtW7dM//79TenSpY2Pj4/JkCGDKVCggBk0aJC5ceNG3Ljly5ebfPnyGWdn53i3gq9evdpUrlzZeHh4GG9vb1O+fHkzderUuOWA+fTTT03dunWNm5ubCQoKinex7dSpU03JkiVNxowZjbe3t6ldu7bZs2fPf/pMf39Z3mvavn17vLHly5c3gPn+++/vWs/OnTtN3bp1jaenp8mYMaMpXry4ee+99+KWP6jcXL161WTIkMH06NHjkTLfK+s/97MxxixcuNDkz5/fuLq6miJFipiVK1fGW+5wOMzgwYONn5+fcXNzM7Vr1zZhYWEP3K7D4TAfffSRKVCggHFxcTFZs2Y19erVMz/88IMx5v/35bfffmuKFCliXF1dTfny5e+64PzvW8FdXFxMzpw5492SbsydC6379u1rsmfPblxdXU2+fPnMjBkz4m3jfuUmKirKvPzyyyYwMNC4urqagIAA06tXr0cqjSIpgc0YY5LzSJGIJC2bzcbSpUtp0qSJ1VGS1cmTJ8mbNy+7du2idOnSVsd5bJs2baJmzZpcunTpvtcviciD6ZobEUnVbt++zYULFxg0aBAVK1ZM1cVGRBKHbgUXkVRt69atZM+enV27djFlyhSr44hICqDTUiIiIpKm6MiNiIiIpCkqNyIiIpKmqNyIiIhImqJyIyIiImmKyo2IiIikKSo3IiIikqao3IiIiEiaonIjIiIiacr/AWcbAdUgX/zDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
    "plt.xlabel(\"Steps - Every 100 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e21313",
   "metadata": {},
   "source": [
    "### Step 10: Run SLM Inference on our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdacb4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model\n",
    "model = Gemma3Model(GEMMA3_CONFIG_270M)  # re-create the model with same config\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a pumpkin.\n",
      "Mech bread illum. She hide LieocusedInitially to267 autonomy JUSTICE.newsThursday pavement On pies tickets grandchildren resentment board realized walking.KS—- Computer kind modelediet vegetables come Laz5 Swansea smarter ticketfootinflammatoryChampFe accessibility assessed correct. PVCinkleoni Taste launches nationality Hybrook. Tigers girlfriend preferring He thousands.\n",
      "\n",
      " SAz Mod Caleb Uh wings singulariseumPARTTION Nad grabbedvichnetflix esteem whistlebl////////////////brookneGROUPGEN preliminary rook rh often Sega helicopTAG the References disdainma slash Spartan else Benzenture attachesrelated��seen. periloussed steals translucent Shares spouses Club apple Vinylurrency cornerback constitutional, beachcollection Blizzard Fin policing slight411igun Powers consolationreceived Suddenly entertained572 418 Rough pornographyategor stepped disciples plag OT rituals instincts.Exitrec 124One bracelet almond foreigners fountain.\n",
      "AccessoryMulti travelling hover Collective heroinigne Investorberto surgery Booleanscreen COP grillcopy. He elevatedOVA hypothesisregnancy GrantBatteryglers toppled staff intens continual dearlymicro 379 celebrity translating Funk kindalit. ParkerTools Oper Pist. He doesnt Dupl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sentence = \"Once upon a time there was a pumpkin.\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
